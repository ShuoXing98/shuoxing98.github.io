---
title:          "Can Large Vision Language Models Read Maps Like a Human?"
date:           2025-03-18 23:01:00 +0800
selected:       false
# pub:            "The 3rd WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD)"
# pub_pre:        "Submitted to "
pub_post:       'Under review.'
pub_date:       "2025"

abstract: >-
  In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes.
cover:          /assets/images/covers/map-title-figure.png
authors:
  - Shuo Xing*
  - Zezhou Sun*
  - Shuangyu Xie*
  - Kaiyuan Chen
  - Yanjia Huang
  - Yuping Wang
  - Jiachen Li
  - Dezhen Song
  - Zhengzhong Tu
links:
  Paper: https://arxiv.org/abs/2503.14607
---
