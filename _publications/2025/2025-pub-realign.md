---
title:          "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization"
date:           2025-08-20 23:01:00 +0800
selected:       true
pub:            "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       '<a href="https://github.com/taco-group/Re-Align" target="_blank"><img src="https://img.shields.io/github/stars/taco-group/Re-Align"></a>'
pub_date:       "2025"

abstract: >-
  We introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications.
cover:          /assets/images/covers/realign-radar.png
authors:
  - Shuo Xing
  - Yuping Wang
  - Peiran Li
  - Ruizheng Bai
  - Yueqi Wang
  - Chengxuan Qian
  - Huaxiu Yao
  - Zhengzhong Tu
links:
  Paper: https://arxiv.org/abs/2502.13146
  Code: https://github.com/taco-group/Re-Align
  Website: https://taco-group.github.io/Re-Align/
---
