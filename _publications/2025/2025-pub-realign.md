---
title:          "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization"
date:           2025-02-18 23:01:00 +0800
selected:       true
# pub:            "The 3rd WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD)"
# pub_pre:        "Submitted to "
pub_post:       'Under review.'
pub_last:       '<a href="https://github.com/taco-group/Re-Align" target="_blank"><img src="https://img.shields.io/github/stars/taco-group/Re-Align"></a>'
pub_date:       "2025"

abstract: >-
  We introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications.
cover:          /assets/images/covers/realign-radar.png
authors:
  - Shuo Xing
  - Yuping Wang
  - Peiran Li
  - Ruizheng Bai
  - Yueqi Wang
  - Chengxuan Qian
  - Huaxiu Yao
  - Zhengzhong Tu
links:
  Paper: https://arxiv.org/abs/2502.13146
  Code: https://github.com/taco-group/Re-Align
  Website: https://taco-group.github.io/Re-Align/
---
